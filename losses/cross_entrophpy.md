# Binary cross entropy

This is alos called as log loss. 
Log loss is useful, when you penalize the probability difference. The small value will get less loss and big difference will bigger loss. 

### Corrected values. 
